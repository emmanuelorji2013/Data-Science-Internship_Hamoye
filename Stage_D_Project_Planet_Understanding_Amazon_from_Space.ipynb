{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stage-D-Project_ Planet_Understanding-Amazon-from-Space.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emmanuelorji2013/Data-Science-Internship_Hamoye/blob/master/Stage_D_Project_Planet_Understanding_Amazon_from_Space.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfyrQ7QpuWWR",
        "colab_type": "text"
      },
      "source": [
        "# Planet: Understanding Amazon from Space\n",
        "Implemented using KFold and VGG19 as the pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2_wLLJ-SpSK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGVgYqiPircX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#upload the credentials of the kaggle account\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AI9wn3d5m5c0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The Kaggle APU expects kaggle.json to be in ~/.kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "\n",
        "# This permission change avoids a warning on Kaggle tool startup\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0HAnYupm9yc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!kaggle datasets download  nikitarom/planets-dataset/ "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBw2u1tqnBcc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#unzipping the zip files and deleting the zip files\n",
        "!unzip \\*.zip  && rm *.zip\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0l81W9b0aTI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir weights # Weights will be saved here for model checkpoint operation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-B23enBl_EV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os, glob, shutil\n",
        "\n",
        "# Combine test images into 1 folder (test-jpg)\n",
        "srcDir = \"test-jpg-additional/test-jpg-additional\"\n",
        "destDir = \"planet/planet/test-jpg\"\n",
        "for filePath in glob.glob(srcDir + '/*'):\n",
        "  # Move each file to destination Directory\n",
        "  shutil.move(filePath, destDir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZAAfJLB1MBc",
        "colab_type": "text"
      },
      "source": [
        "## Import Necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jt-GFTt2mfIM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import fbeta_score\n",
        "\n",
        "from keras.applications.vgg19 import VGG19\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, BatchNormalization\n",
        "\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# Initializations\n",
        "epochs = 50\n",
        "batch_size = 128\n",
        "\n",
        "input_size = 128\n",
        "input_channels = 3\n",
        "\n",
        "n_folds = 5\n",
        "\n",
        "training = True\n",
        "\n",
        "ensemble_voting = False  # If True, use voting for model ensemble, otherwise use averaging"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7y707owgxHp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_im_names = [os.path.splitext(filename)[0] for filename in os.listdir('planet/planet/test-jpg')]\n",
        "df_train_data = pd.read_csv('planet/planet/train_classes.csv')\n",
        "df_test_data = pd.DataFrame({ 'image_name': sorted(test_im_names), 'tags': \"\"})\n",
        "\n",
        "# Make a list of all possible labels\n",
        "flatten = lambda l: [item for sublist in l for item in sublist]\n",
        "labels = list(set(flatten([l.split(' ') for l in df_train_data['tags'].values])))\n",
        "\n",
        "# Dictionary mapping labels to integer values 0-16\n",
        "map_labels = {l: i for i, l in enumerate(labels)}\n",
        "inv_map_labels = {i: l for l, i in map_labels.items()} # Inversion between keys and values in map_labels\n",
        "\n",
        "kfold = KFold(n_splits=n_folds, shuffle=True, random_state=1)\n",
        "\n",
        "fold_count = 0\n",
        "\n",
        "y_full_test = []\n",
        "thres_sum = np.zeros(17, np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LW8eN8s3LoR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"There're {} labels\\n\".format(len(labels)))\n",
        "labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8G-8tCI2AL1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_test_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tz-juiY40yRE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XAhdwOy5Sfr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "map_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwWh6I_b6NOg",
        "colab_type": "text"
      },
      "source": [
        "## Split KFolds and train each fold"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nb73Ko1foqqp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for train_index, test_index in kfold.split(df_train_data):\n",
        "\n",
        "    fold_count += 1\n",
        "    print('Fold ', fold_count)\n",
        "\n",
        "    df_train = df_train_data.loc[train_index]\n",
        "    if training:\n",
        "        print('----Will trian with {} samples ----'.format(len(df_train)))\n",
        "\n",
        "    df_valid = df_train_data.loc[test_index]\n",
        "    print('----Will validate with {} samples '.format(len(df_valid)))\n",
        "\n",
        "    #Custom train data generator\n",
        "    def train_generator():\n",
        "        while True:\n",
        "            for start in range(0, len(df_train), batch_size):\n",
        "                batch_x = []\n",
        "                batch_y = []\n",
        "                stop = min(start + batch_size, len(df_train))\n",
        "                df_train_batch = df_train[start:stop]\n",
        "                for f, tags in df_train_batch.values:\n",
        "                    image = cv2.imread('planet/planet/train-jpg/{}.jpg'.format(f))\n",
        "                    image = cv2.resize(image, (input_size, input_size))\n",
        "                    image = transformations(image, np.random.randint(6))\n",
        "                    targets = np.zeros(17)\n",
        "                    for t in tags.split(' '):\n",
        "                        targets[map_labels[t]] = 1\n",
        "                    batch_x.append(image)\n",
        "                    batch_y.append(targets)\n",
        "                batch_x = np.array(batch_x, np.float32)\n",
        "                batch_y = np.array(batch_y, np.uint8)\n",
        "                yield batch_x, batch_y\n",
        "\n",
        "    #Custom validation data generator\n",
        "    def valid_generator():\n",
        "        while True:\n",
        "            for start in range(0, len(df_valid), batch_size):\n",
        "                batch_x = []\n",
        "                batch_y = []\n",
        "                stop = min(start + batch_size, len(df_valid))\n",
        "                df_valid_batch = df_valid[start:stop]\n",
        "                for f, tags in df_valid_batch.values:\n",
        "                    image = cv2.imread('planet/planet/train-jpg/{}.jpg'.format(f))\n",
        "                    image = cv2.resize(image, (input_size, input_size))\n",
        "                    image = transformations(image, np.random.randint(6))\n",
        "                    targets = np.zeros(17)\n",
        "                    for t in tags.split(' '):\n",
        "                        targets[map_labels[t]] = 1\n",
        "                    batch_x.append(image)\n",
        "                    batch_y.append(targets)\n",
        "                batch_x = np.array(batch_x, np.float32)\n",
        "                batch_y = np.array(batch_y, np.uint8)\n",
        "                yield batch_x, batch_y\n",
        "\n",
        "\n",
        "    #Transformations for data augumentation\n",
        "    def transformations(src, choice):\n",
        "        if choice == 0:\n",
        "            # Rotate 90\n",
        "            src = cv2.rotate(src, rotateCode=cv2.ROTATE_90_CLOCKWISE)\n",
        "        if choice == 1:\n",
        "            # Rotate 90 and flip horizontally\n",
        "            src = cv2.rotate(src, rotateCode=cv2.ROTATE_90_CLOCKWISE)\n",
        "            src = cv2.flip(src, flipCode=1)\n",
        "        if choice == 2:\n",
        "            # Rotate 180\n",
        "            src = cv2.rotate(src, rotateCode=cv2.ROTATE_180)\n",
        "        if choice == 3:\n",
        "            # Rotate 180 and flip horizontally\n",
        "            src = cv2.rotate(src, rotateCode=cv2.ROTATE_180)\n",
        "            src = cv2.flip(src, flipCode=1)\n",
        "        if choice == 4:\n",
        "            # Rotate 90 counter-clockwise\n",
        "            src = cv2.rotate(src, rotateCode=cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
        "        if choice == 5:\n",
        "            # Rotate 90 counter-clockwise and flip horizontally\n",
        "            src = cv2.rotate(src, rotateCode=cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
        "            src = cv2.flip(src, flipCode=1)\n",
        "        return src\n",
        "\n",
        "\n",
        "\n",
        "    base_model = VGG19(include_top=False,\n",
        "                       weights='imagenet',\n",
        "                       input_shape=(input_size, input_size, input_channels))\n",
        "\n",
        "    model = Sequential()\n",
        "    # Batchnorm input\n",
        "    model.add(BatchNormalization(input_shape=(input_size, input_size, input_channels)))\n",
        "    # Base model\n",
        "    model.add(base_model)\n",
        "    # Classifier\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(17, activation='sigmoid'))\n",
        "\n",
        "    opt = Adam(lr=1e-4)\n",
        "\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  # We NEED binary here, since categorical_crossentropy l1 norms the output before calculating loss.\n",
        "                  optimizer=opt,\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    print(\"CallBacks\")\n",
        "    callbacks = [EarlyStopping(monitor='val_loss',\n",
        "                               patience=4,\n",
        "                               verbose=1,\n",
        "                               min_delta=1e-4),\n",
        "                 ReduceLROnPlateau(monitor='val_loss',\n",
        "                                   factor=0.1,\n",
        "                                   patience=2,\n",
        "                                   cooldown=2,\n",
        "                                   verbose=1),\n",
        "                 ModelCheckpoint(filepath='weights/best_weights.fold_' + str(fold_count) + '.hdf5',\n",
        "                                 save_best_only=True,\n",
        "                                 save_weights_only=True)]\n",
        "\n",
        "    print(\"Training\")\n",
        "    if training:\n",
        "        model.fit(x=train_generator(),\n",
        "                            steps_per_epoch=(len(df_train) // batch_size) + 1,\n",
        "                            epochs=epochs,\n",
        "                            verbose=2,\n",
        "                            callbacks=callbacks,\n",
        "                            validation_data=valid_generator(),\n",
        "                            validation_steps=(len(df_valid) // batch_size) + 1)\n",
        "\n",
        "\n",
        "    def optimise_f2_thresholds(y, p, verbose=True, resolution=100):\n",
        "        def mf(x):\n",
        "            p2 = np.zeros_like(p)\n",
        "            for i in range(17):\n",
        "                p2[:, i] = (p[:, i] > x[i]).astype(np.int)\n",
        "            score = fbeta_score(y, p2, beta=2, average='samples')\n",
        "            return score\n",
        "\n",
        "        x = [0.2] * 17\n",
        "        for i in range(17):\n",
        "            best_i2 = 0\n",
        "            best_score = 0\n",
        "            for i2 in range(resolution):\n",
        "                i2 /= float(resolution)\n",
        "                x[i] = i2\n",
        "                score = mf(x)\n",
        "                if score > best_score:\n",
        "                    best_i2 = i2\n",
        "                    best_score = score\n",
        "            x[i] = best_i2\n",
        "            if verbose:\n",
        "                print(i, best_i2, best_score)\n",
        "        return x\n",
        "\n",
        "    print(\"Load Weights\")\n",
        "    # Load best weights\n",
        "    model.load_weights(filepath='weights/best_weights.fold_' + str(fold_count) + '.hdf5')\n",
        "\n",
        "    valid_pred = model.predict(x=valid_generator(),\n",
        "                                      steps=(len(df_valid) // batch_size) + 1)\n",
        "\n",
        "    valid_y = []\n",
        "    for f, tags in df_valid.values:\n",
        "        targets = np.zeros(17)\n",
        "        for t in tags.split(' '):\n",
        "            targets[map_labels[t]] = 1\n",
        "        valid_y.append(targets)\n",
        "    valid_y = np.array(valid_y, np.uint8)\n",
        "\n",
        "    # Find optimal f2 thresholds for local validation set\n",
        "    thres = optimise_f2_thresholds(valid_y, valid_pred, verbose=False)\n",
        "\n",
        "    print('F2 = {}'.format(fbeta_score(valid_y, np.array(valid_pred) > thres, beta=2, average='samples')))\n",
        "\n",
        "    thres_sum += np.array(thres, np.float32)\n",
        "\n",
        "\n",
        "    def test_generator(transformation):\n",
        "        \n",
        "        while True:\n",
        "            for start in range(0, len(df_test_data), batch_size):\n",
        "                batch_x = []\n",
        "                stop = min(start + batch_size, len(df_test_data))\n",
        "                df_test_batch = df_test_data[start:stop]\n",
        "                for f, tags in df_test_batch.values:\n",
        "                    image = cv2.imread('planet/planet/test-jpg/{}.jpg'.format(f))\n",
        "                    image = cv2.resize(image, (input_size, input_size))\n",
        "                    image = transformations(image, transformation)\n",
        "                    batch_x.append(image)\n",
        "                    \n",
        "                batch_x = np.array(batch_x, np.float32)\n",
        "                yield batch_x\n",
        "\n",
        "    # 6-fold Test Time Augmentation\n",
        "    p_full_test = []\n",
        "    for i in range(6):\n",
        "      p_test = model.predict(x=test_generator(transformation=i),\n",
        "                                        steps=(len(df_test_data) // batch_size) + 1)\n",
        "      p_full_test.append(p_test)\n",
        "      \n",
        "\n",
        "    p_test = np.array(p_full_test[0])\n",
        "    for i in range(1, 6):\n",
        "        p_test += np.array(p_full_test[i])\n",
        "    p_test /= 6\n",
        "\n",
        "    y_full_test.append(p_test)\n",
        "\n",
        "result = np.array(y_full_test[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tk-OnVx37msO",
        "colab_type": "text"
      },
      "source": [
        "## Model Ensemble"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbmQplAN8Lkm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if voting_ensemble:\n",
        "    for f in range(len(y_full_test[0])):  # For each file\n",
        "        for tag in range(17):  # For each tag\n",
        "            preds = []\n",
        "            for fold in range(n_folds):  # For each fold\n",
        "                preds.append(y_full_test[fold][f][tag])\n",
        "            pred = Counter(preds).most_common(1)[0][0]  # Most common tag prediction among folds\n",
        "            result[f][tag] = pred\n",
        "else:\n",
        "    for fold in range(1, n_folds):\n",
        "        result += np.array(y_full_test[fold])\n",
        "    result /= n_folds\n",
        "result = pd.DataFrame(result, columns=labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvO2AllOUBDx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds = []\n",
        "thres = (thres_sum / n_folds).tolist()\n",
        "\n",
        "for i in tqdm(range(result.shape[0]), miniters=1000):\n",
        "    a = result.loc[[i]]\n",
        "    a = a.apply(lambda x: x > thres, axis=1)\n",
        "    a = a.transpose()\n",
        "    a = a.loc[a[i] == True]\n",
        "    ' '.join(list(a.index))\n",
        "    preds.append(' '.join(list(a.index)))\n",
        "\n",
        "df_test_data['tags'] = preds\n",
        "submsnDest = r\"/content/drive/My Drive/Colab Notebooks/Hamoye Internship/\"\n",
        "df_test_data.to_csv(\"{}/submission.csv\".format(submsnDest), index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}